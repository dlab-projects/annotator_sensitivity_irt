{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_lego as mplego\n",
    "import pandas as pd\n",
    "\n",
    "from hatespeech import keys, utils\n",
    "from mpl_lego.colorbar import append_colorbar_to_axis\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplego.style.use_latex_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_diffs = pd.read_csv('severity_diffs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"22\"\n",
    "base_path = os.path.join(os.environ['HOME'], 'projects/annotator_bias_irt')\n",
    "data_path = \"~/data/hatespeech/unfiltered_ratings.feather\"\n",
    "rater_quality_path = \"~/data/hatespeech/rater_quality_check.csv\"\n",
    "results_path = os.path.join(base_path, f'scaling/experiments/exp{exp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(data_path).rename(columns={'violence_phys': 'violence'})\n",
    "# Remove all rows in which some item is missing\n",
    "data = utils.filter_missing_items(data)\n",
    "# Remove all rows in which the rater is not up to sufficient quality\n",
    "rater_quality = pd.read_csv(rater_quality_path)\n",
    "data = utils.filter_annotator_quality(data, rater_quality)\n",
    "# Recode item responses\n",
    "data = utils.recode_responses(\n",
    "    data,\n",
    "    insult={1: 0, 2: 1, 3: 2, 4: 3},\n",
    "    humiliate={1: 0, 2: 0, 3: 1, 4: 2},\n",
    "    status={1: 0, 2: 0, 3: 1, 4: 1},\n",
    "    dehumanize={1: 0, 2: 0, 3: 1, 4: 1},\n",
    "    violence={1: 0, 2: 0, 3: 1, 4: 1},\n",
    "    genocide={1: 0, 2: 0, 3: 1, 4: 1},\n",
    "    attack_defend={1: 0, 2: 1, 3: 2, 4: 3},\n",
    "    hatespeech={1: 0, 2: 1})\n",
    "# Only get comments targeting black / white people\n",
    "data = data[data['target_race_white'] | data['target_race_black']]\n",
    "data = data[data[keys.target_race_cols].sum(axis=1) == 1]\n",
    "data['target_race'] = np.where(data['target_race_white'], 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in experiment output\n",
    "out_path = os.path.join(results_path, f\"exp{exp}_out.txt\")\n",
    "with open(out_path) as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_lines = lines[17588:30364]\n",
    "bias_cut = bias_lines[4:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract elements of each line\n",
    "n_samples = len(bias_cut)\n",
    "observed = np.zeros(n_samples)\n",
    "expected = np.zeros(n_samples)\n",
    "bias_size = np.zeros(n_samples)\n",
    "t_stats = np.zeros(n_samples)\n",
    "p_vals = np.zeros(n_samples)\n",
    "labeler_id = np.zeros(n_samples)\n",
    "measure = np.zeros(n_samples)\n",
    "race = np.zeros(n_samples)\n",
    "\n",
    "for idx, line in enumerate(bias_cut):\n",
    "    processed = line.replace('|', ' ').replace('>', ' ').replace('<', ' ').split()\n",
    "    observed[idx] = float(processed[0])\n",
    "    expected[idx] = float(processed[1])\n",
    "    bias_size[idx] = float(processed[4])\n",
    "    t_stats[idx] = float(processed[6])\n",
    "    p_vals[idx] = float(processed[8])\n",
    "    labeler_id[idx] = int(processed[13])\n",
    "    measure = float(processed[14])\n",
    "    if processed[16] == 'white':\n",
    "        race[idx] = 0\n",
    "    elif processed[16] == 'black':\n",
    "        race[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results data frame\n",
    "results = pd.DataFrame({\n",
    "    'labeler_id': labeler_id,\n",
    "    'observed': observed,\n",
    "    'expected': expected,\n",
    "    'bias': bias_size,\n",
    "    't_statistic': t_stats,\n",
    "    'p_value': p_vals,\n",
    "    'measure': measure,\n",
    "    'race': race}).astype({'labeler_id': int})\n",
    "results['bias_corrected'] = -results['bias']\n",
    "results['bias_abs'] = results['bias'].abs()\n",
    "results['target_race_name'] = np.where(results['race'] == 1, 'black', 'white')\n",
    "# Merge in annotator race\n",
    "results = results.merge(\n",
    "    right=data[['labeler_id'] + keys.annotator_race_cols].drop_duplicates('labeler_id'),\n",
    "    how='left',\n",
    "    on='labeler_id')\n",
    "# Add in bias sign\n",
    "results['bias_sign'] = np.where(\n",
    "    results['bias_corrected'] < 0,\n",
    "    'negative',\n",
    "    np.where(\n",
    "        results['bias_corrected'] > 0,\n",
    "        'positive',\n",
    "        'zero'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_targets = results[results['target_race_name'] == 'black']\n",
    "black_bias = black_targets[black_targets['labeler_id'].isin(severity_diffs['labeler_id'])].sort_values('labeler_id')[['labeler_id', 'bias_corrected']]\n",
    "white_targets = results[results['target_race_name'] == 'white']\n",
    "white_bias = white_targets[white_targets['labeler_id'].isin(severity_diffs['labeler_id'])].sort_values('labeler_id')[['labeler_id', 'bias_corrected']]\n",
    "all_bias = black_bias.merge(white_bias, how='inner', on='labeler_id', suffixes=('_black', '_white'))\n",
    "all_bias['interaction_diffs'] = all_bias['bias_corrected_black'] - all_bias['bias_corrected_white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = all_bias[['labeler_id', 'interaction_diffs']].merge(severity_diffs, on='labeler_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "img = ax.hexbin(\n",
    "    diffs['severity_diffs'],\n",
    "    diffs['interaction_diffs'],\n",
    "    cmap='Greys',\n",
    "    bins='log',\n",
    "    extent=(-6, 6, -6, 6),\n",
    "    gridsize=30)\n",
    "\n",
    "ax.set_xlim([-6, 6])\n",
    "ax.set_ylim([-6, 6])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xticks([-6, -3, 0, 3, 6])\n",
    "ax.set_yticks([-6, -3, 0, 3, 6])\n",
    "ax.set_xlabel(r'\\textbf{Annotator Lean}', fontsize=20)\n",
    "ax.set_ylabel(r'\\textbf{Interaction Difference}', fontsize=20)\n",
    "ax.tick_params(labelsize=18)\n",
    "cb, cax = append_colorbar_to_axis(ax, img)\n",
    "cb.ax.tick_params(labelsize=15)\n",
    "cb.set_label(mplego.labels.bold_text('Number of Annotators'), rotation=270, fontsize=15, labelpad=20)\n",
    "plt.savefig('figureA6.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate",
   "language": "python",
   "name": "hate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
